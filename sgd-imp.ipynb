{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Définir la fonction H(x, y) et son gradient\ndef H(x, y):\n    return (x - 2)**2 + 5 * (x**2 - y)**2 + 3 * (y - 1)**2\n\ndef gradient_H(x, y):\n    grad_x = 2 * (x - 2) + 10 * x * (x**2 - y)\n    grad_y = -10 * (x**2 - y) + 6 * (y - 1)\n    return np.array([grad_x, grad_y])\n\n# Implémentation de SGD Stochastique\ndef stochastic_gradient_descent(x0, y0, alpha=0.01, tol=1e-6, max_iter=1000):\n    x, y = x0, y0\n    path = [(x, y)]\n    start_time = time.time()\n    iterations = 0\n\n    for _ in range(max_iter):\n        grad = gradient_H(np.random.uniform(x-0.5, x+0.5), np.random.uniform(y-0.5, y+0.5))\n        x -= alpha * grad[0]\n        y -= alpha * grad[1]\n        path.append((x, y))\n        iterations += 1\n        if np.linalg.norm(grad) < tol:\n            break\n\n    execution_time = time.time() - start_time\n    return (x, y), path, iterations, execution_time\n\n# Implémentation de SGD avec Momentum Stochastique\ndef sgd_momentum(x0, y0, alpha=0.01, beta=0.9, tol=1e-6, max_iter=1000):\n    x, y = x0, y0\n    vx, vy = 0, 0\n    path = [(x, y)]\n    start_time = time.time()\n    iterations = 0\n\n    for _ in range(max_iter):\n        grad = gradient_H(np.random.uniform(x-0.5, x+0.5), np.random.uniform(y-0.5, y+0.5))\n        vx = beta * vx - alpha * grad[0]\n        vy = beta * vy - alpha * grad[1]\n        x += vx\n        y += vy\n        path.append((x, y))\n        iterations += 1\n        if np.linalg.norm(grad) < tol:\n            break\n\n    execution_time = time.time() - start_time\n    return (x, y), path, iterations, execution_time\n\n# Implémentation de l'algorithme Adam Stochastique\ndef adam(x0, y0, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, tol=1e-6, max_iter=1000):\n    x, y = x0, y0\n    m = np.zeros(2)  # Moyenne des gradients\n    v = np.zeros(2)  # Variance des gradients\n    path = [(x, y)]\n    start_time = time.time()\n    iterations = 0\n\n    for t in range(1, max_iter + 1):\n        grad = gradient_H(np.random.uniform(x-0.5, x+0.5), np.random.uniform(y-0.5, y+0.5))\n        m = beta1 * m + (1 - beta1) * grad\n        v = beta2 * v + (1 - beta2) * (grad**2)\n        m_hat = m / (1 - beta1**t)\n        v_hat = v / (1 - beta2**t)\n        x -= alpha * m_hat[0] / (np.sqrt(v_hat[0]) + epsilon)\n        y -= alpha * m_hat[1] / (np.sqrt(v_hat[1]) + epsilon)\n        path.append((x, y))\n        iterations += 1\n        if np.linalg.norm(grad) < tol:\n            break\n\n    execution_time = time.time() - start_time\n    return (x, y), path, iterations, execution_time\n\n# Comparaison des méthodes\ndef compare_methods():\n    # Point de départ choisi aléatoirement dans [-2, 2]\n    x0, y0 = np.random.uniform(-2, 2), np.random.uniform(-2, 2)\n    print(f\"Point de départ choisi aléatoirement : x0 = {x0:.2f}, y0 = {y0:.2f}\")\n    \n    tol = 1e-6  # Tolérance\n    max_iter = 1000  # Nombre maximal d'itérations\n\n    # SGD Stochastique\n    minimum_sgd, path_sgd, iterations_sgd, time_sgd = stochastic_gradient_descent(x0, y0, alpha=0.01, tol=tol, max_iter=max_iter)\n\n    # SGD avec Momentum Stochastique\n    minimum_momentum, path_momentum, iterations_momentum, time_momentum = sgd_momentum(x0, y0, alpha=0.01, beta=0.9, tol=tol, max_iter=max_iter)\n\n    # Adam Stochastique\n    minimum_adam, path_adam, iterations_adam, time_adam = adam(x0, y0, alpha=0.01, tol=tol, max_iter=max_iter)\n\n    # Affichage des résultats\n    print(\"Comparaison des Méthodes :\")\n    print(f\"SGD Stochastique : {iterations_sgd} itérations, Temps : {time_sgd:.4f} s, Minimum : {minimum_sgd}\")\n    print(f\"SGD avec Momentum Stochastique : {iterations_momentum} itérations, Temps : {time_momentum:.4f} s, Minimum : {minimum_momentum}\")\n    print(f\"Adam Stochastique : {iterations_adam} itérations, Temps : {time_adam:.4f} s, Minimum : {minimum_adam}\")\n\n    # Création de sous-graphiques pour chaque méthode\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n    # Trajectoire SGD\n    path_sgd = np.array(path_sgd)\n    axes[0].plot(path_sgd[:, 0], path_sgd[:, 1], label=\"SGD Stochastique\", marker='o', linestyle='-')\n    axes[0].set_title(\"SGD Stochastique\")\n    axes[0].set_xlabel(\"x\")\n    axes[0].set_ylabel(\"y\")\n    axes[0].grid(True)\n    axes[0].legend()\n\n    # Trajectoire SGD avec Momentum\n    path_momentum = np.array(path_momentum)\n    axes[1].plot(path_momentum[:, 0], path_momentum[:, 1], label=\"SGD Momentum Stochastique\", marker='x', linestyle='--')\n    axes[1].set_title(\"SGD Momentum Stochastique\")\n    axes[1].set_xlabel(\"x\")\n    axes[1].set_ylabel(\"y\")\n    axes[1].grid(True)\n    axes[1].legend()\n\n    # Trajectoire Adam\n    path_adam = np.array(path_adam)\n    axes[2].plot(path_adam[:, 0], path_adam[:, 1], label=\"Adam Stochastique\", marker='s', linestyle='-.')\n    axes[2].set_title(\"Adam Stochastique\")\n    axes[2].set_xlabel(\"x\")\n    axes[2].set_ylabel(\"y\")\n    axes[2].grid(True)\n    axes[2].legend()\n\n    # Ajuster l'affichage\n    plt.tight_layout()\n    plt.show()\n\n# Lancer la comparaison\ncompare_methods()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:28:52.259841Z","iopub.execute_input":"2024-12-16T20:28:52.260198Z","iopub.status.idle":"2024-12-16T20:28:53.255817Z","shell.execute_reply.started":"2024-12-16T20:28:52.260165Z","shell.execute_reply":"2024-12-16T20:28:53.254786Z"}},"outputs":[],"execution_count":null}]}